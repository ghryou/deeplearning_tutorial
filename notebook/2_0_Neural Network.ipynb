{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "---\n",
    "\n",
    "\\begin{align} y = f(w^T\\phi(x)) \\end{align}\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 \\end{align}\n",
    "\n",
    "<img src=\"https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/Single-Perceptron.png?x31195\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation with previous model\n",
    "---\n",
    "\n",
    "\\begin{align} y = f(w^T\\phi(x)) \\end{align}\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 \\end{align}\n",
    "\n",
    "\n",
    "### Perceptron Rule for SVM\n",
    "\n",
    "\\begin{align} y = sgn(w^{*T}x+b^*) \\end{align}\n",
    "\n",
    "\\begin{align} E_p(w) = -\\sum y_iw^Tx_i \\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/2012mdsp-pr13supportvectormachine-130701022429-phpapp02/95/2012mdsp-pr13-support-vector-machine-5-638.jpg?cb=1372645656\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function\n",
    "---\n",
    "\n",
    "### Probabilistic Supervised Learning - Logistic Regression\n",
    "\n",
    "![](http://www.saedsayad.com/images/LogReg_1.png)\n",
    "\n",
    "\\begin{align} p(y = 1 |x; \\theta) = \\sigma(\\theta^Tx) \\end{align}\n",
    "\n",
    "\\begin{align} \\theta_{ML} = \\underset{\\theta}{\\operatorname{argmax}} P(Y|X; \\theta) \\end{align}\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "\\begin{align} \\theta_{ML} = \\underset{\\theta}{\\operatorname{argmax}} P(Y|X; \\theta) \\end{align}\n",
    "\n",
    "\\begin{align} P(Y|X; \\theta) = \\displaystyle\\prod_{i=1} P(y_i|x_i; \\theta) \\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\theta_{ML} &= \\underset{\\theta}{\\operatorname{argmin}} \\Big(-\\log P(Y|X; \\theta)\\Big) \\\\\n",
    "            &= \\underset{\\theta}{\\operatorname{argmin}} \\Big(-\\displaystyle\\sum_{i=1}{\\log P(y_i|x_i; \\theta)} \\Big)\n",
    "\\end{align}\n",
    "\n",
    "<center>Negative Log Likelihood loss (NLL_loss)</center>\n",
    "\n",
    "\n",
    "### Information Theory\n",
    "\n",
    "- Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content what so ever.\n",
    "- Less likely events should have higher information content.\n",
    "- Independent events should have additive information.\n",
    "\n",
    "\\begin{align} I(X) = -logP(X) \\end{align}\n",
    "\n",
    "\n",
    "### Shannon's Entropy\n",
    "\n",
    "\\begin{align} H(X) = E_{x \\sim P}[I(X)] \\end{align}\n",
    "\n",
    "<img src=\"https://i.imgur.com/Pynf9sG.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "### Cross-Entropy\n",
    "\n",
    "데이터 분포 P(X) 상에서 추정 확률 분포 Q(X)가 가지는 정보의 기댓값\n",
    "\n",
    "\\begin{align} H(P,Q) = H(P) + D_{KL}(P||Q) \\end{align}\n",
    "\n",
    "\\begin{align} H(P,Q) = E_{x \\sim P}[\\log Q(X)] \\end{align}\n",
    "\n",
    "\n",
    "### Kullback-Leibler divergence, KLD\n",
    "\n",
    "두 확률분포의 차이를 계산하는 데 사용하는 함수. 가지고 있는 데이터의 분포 P(x)와 모델이 추정한 데이터의 분포 Q(x) 간의 차이\n",
    "\n",
    "\\begin{align} D_{KL}(P||Q) = E_{x \\sim P}[\\log\\frac{P(X)}{Q(X)}] \\end{align}\n",
    "\n",
    "\n",
    "### Relation between Cross Entropy Loss and Maximum Likelihood Estimation\n",
    "\n",
    "\\begin{align} H(P,Q) = E_{x \\sim P}[\\log Q(X)] \\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\theta_{ML} &= \\underset{\\theta}{\\operatorname{argmax}} H(P_{real},P_{train}) \\\\\n",
    "            &= \\underset{\\theta}{\\operatorname{argmax}} E_{x \\sim P}[\\log Q(X)] \\\\\n",
    "            &\\sim \\underset{\\theta}{\\operatorname{argmax}} \\Big(-\\displaystyle\\sum_{i=1}{\\log P(y_i|x_i; \\theta)} \\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation\n",
    "---\n",
    "\n",
    "<a href=\"http://refopen.blogspot.com/2014/08/blog-post_67.html\">Reference: 신경망 회로에서 역전파를 통한 학습</a>\n",
    "\n",
    "<a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\">PyTorch Autograd</a>\n",
    "\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-xG4QVNhee6Q/U_2Sl3mNoAI/AAAAAAAAAXI/YdGVn0WUOi4/s1600/nn001%2B(2).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://3.bp.blogspot.com/-HBfLo3KYaJU/U_2Snq1G3vI/AAAAAAAAAXY/OVz2nwLXpvo/s1600/nn001%2B(6).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-R3HOr1Q6v5A/U_2Sn3GAbMI/AAAAAAAAAXg/9omSLIyyKjc/s1600/nn001%2B(7).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://3.bp.blogspot.com/-dHGoeyo6DvA/U_2SoR7IEqI/AAAAAAAAAXo/ByirJ619-Ow/s1600/nn001%2B(8).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://3.bp.blogspot.com/-cB03ynPZidI/U_2SlhYxC-I/AAAAAAAAAW4/kBaa01WhpjA/s1600/nn001%2B(19).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://2.bp.blogspot.com/-F4rno0XjmTg/U_2So8TA7iI/AAAAAAAAAXw/fJwLynE9Fxc/s1600/nn001%2B(9).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-23z13NYIGlo/U_2Sk2w6zTI/AAAAAAAAAWs/-IajPm1Z6xg/s1600/nn001%2B(17).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://4.bp.blogspot.com/-ybPggV2Z7ws/U_2Si4eGzLI/AAAAAAAAAWY/RVNyacmHTaE/s1600/nn001%2B(12).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-UoSwLEhVKrY/U_2Sjgg8EjI/AAAAAAAAAWg/lvzUnr-ufF4/s1600/nn001%2B(14).gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/2168AC3758D5CE0C33\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*RD0lIYqB5L2LrI2VTIZqGw.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization\n",
    "---\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/random-170910154045/95/-53-638.jpg?cb=1505089848\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "### Learning Theory - VC Dimension (Vapnik–Chervonenkis dimension)\n",
    "\n",
    "<img src=\"http://www.guochaoping.top/wp-content/uploads/2016/11/Sample_complexity_tradeoff.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\\begin{align} error_{test} \\leq error_{training} + \\sqrt{\\frac{1}{N}\\Big[D\\Big(\\log\\Big(\\frac{2N}{D}\\Big)+1\\Big)-\\log\\Big(\\frac{1}{\\delta}\\Big)\\Big]} \\end{align}\n",
    "\n",
    "\\begin{align} d_{VC} = \\frac{D}{N}, \\:\\: D:\\:model\\:complexity, \\:\\: N:\\:number\\:of\\:samples \\end{align}\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-03-at-7.52.01-PM-e1522832332857.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 + \\lambda ||\\theta||^2 \\end{align}\n",
    "\n",
    "\\begin{align} h_\\theta(x_i) = \\theta^Tx_i + b \\end{align}\n",
    "\n",
    "\\begin{align} \\theta^* = \\arg\\min_\\theta J(\\theta) \\end{align}\n",
    "\n",
    "\n",
    "### Drop Out\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/random-170910154045/95/-55-638.jpg?cb=1505089848\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*vW3KGPp_w0wN6k3gYVlVHA.jpeg\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced\">Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "### Hyperparamter Training - Cross Validation\n",
    "\n",
    "<img src=\"http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/08_CV_Ensembling/fig-holdout.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/1fXzJ.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Techniques\n",
    "---\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*_Mqsdev-vcII6xyyOeIUvA.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "<img src=\"https://shuuki4.files.wordpress.com/2016/01/bn1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/\">Batch Normalization 설명 및 구현</a>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\">Understanding the gradient flow through the batch normalization</a>\n",
    "</center>\n",
    "\n",
    "\\begin{align}\n",
    "y = x w_1 w_2 w_3 \\cdots w_l\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "x(w_1 − \\nabla g_1)(w_2 − \\nabla g_2) \\cdots (w_l − \\nabla g_l)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\epsilon g_1 \\prod_{i=2}w_i\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### Continuation Learning & Curriculum Learning\n",
    "\n",
    "\\begin{align} J(\\theta) \\rightarrow \\{J^{(0)}(\\theta), J^{(1)}(\\theta), \\cdots, J^{(n)}(\\theta)\\} \\end{align}\n",
    "\n",
    "\\begin{align} J^{(i)}(\\theta) = E_{\\theta^\\prime∼N(\\theta^\\prime;\\theta,\\sigma(i)^2)}\\big[J(\\theta)\\big] \\end{align}\n",
    "\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/01ae5f0820191ee7774456cb609332269cd3aa3f/3-Figure3-1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/deepmind-live-cms/documents/StreetLearn_Paris_Short.gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://arxiv.org/abs/1804.00168\">Learning to Navigate in Cities Without a Map [P Mirowski et al. (2018)]</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "<a href=\"http://pythonkim.tistory.com/41\">Reference: Weight 초기화 잘해보자, Sung Kim</a>\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/2723A24C57A0076325\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/26191A4C57A007642D\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "#### Unsupervised Pre-training\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/2611A94C57A0076732\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/2320DD4B57A0076D29\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\\begin{align} E(v,h) = -\\sum_{i,j} v_i w_{ij} h_j - \\sum_i b_i^v v_i \\sum_j b_j^h h_j = -v^T W h - b^T(v+h) \\end{align}\n",
    "\n",
    "\\begin{align} v_i\\::\\:visible\\:nodes,\\:h_i\\::\\:hidden\\:nodes \\end{align}\n",
    "\n",
    "\\begin{align} P(v) = \\sum_h P(v,h) = \\frac{\\sum_h exp(-E(v,h))}{Z} \\end{align}\n",
    "\n",
    "\\begin{align} w = \\underset{w}{\\operatorname{argmax}} - \\sum P(X)lnP(X) \\end{align}\n",
    "\n",
    "<center>\n",
    "    <a href=\"http://www.cs.toronto.edu/~fritz/absps/ncfast.pdf\">[Hinton et al. (2006)]</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "#### Xavier & He\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/2777CD4E57A0077436\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/linear.py#L44-L48\">PyTorch nn.linear initialization</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "#### Supervised Pre-training\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/cd85a549add0c7c7def36aca29837efd24b24080/4-Figure1-1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://arxiv.org/pdf/1412.6550.pdf\">FitNets (Romero et al., 2015)</a>\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
