{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Berkeley CS188](http://ai.berkeley.edu/lecture_slides.html)\n",
    "\n",
    "[Berkeley CS294](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html)\n",
    "\n",
    "[Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)\n",
    "\n",
    "\n",
    "MDP (Markov Decision Process)\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/9864ef6a012bcbff9249a3805b06035d.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/b256481449d77879cff9109fbecb08d1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Value Function\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/2f32323a0ff14183c045cfb04744ab73.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### State - Value Function\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/4885d4877f3115bb054016dbd00e14ea.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Action - Value Function (Q Function)\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/e7b067d294a64c295cd120d1cdf33e20.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### Bellman Equation\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/dfq.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/18eba72dcfeafa6e6280055a95078ffa.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Bellman Expectation Equation\n",
    "\\begin{align}\n",
    "v_\\pi(s) &= \\sum_{a\\in \\it{A}} \\pi(a|s)q_\\pi(s,a) \\\\\n",
    "q_\\pi(s,a) &= R(s,a) + \\gamma \\mathbb{E}_\\pi[q_\\pi(S_{t+1}, A_{t+1} | S_t=s, A_t=a)] \\\\\n",
    "       &= R(s,a) + \\gamma \\sum_{s^\\prime\\in \\it{S}} \\mathbb{P}(S_{t+1} = s^\\prime|S_t = s, A_t = a) \\Big( \\sum_{a^\\prime \\in \\it{A}} \\pi(a^\\prime|s^\\prime)q_\\pi(s^\\prime,a^\\prime) \\Big) \\\\\n",
    "       &= R(s,a) + \\gamma \\sum_{s^\\prime\\in \\it{S}} \\mathbb{P}(S_{t+1} = s^\\prime|S_t = s, A_t = a)v_\\pi(s) \\\\\n",
    "       &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_\\pi(s^\\prime) \\\\\n",
    "v_\\pi(s) &= \\sum_{a\\in \\it{A}} \\pi(a|s)\\Big(R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_\\pi(s^\\prime)\\Big) \\\\\n",
    "q_\\pi(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a \\Big( \\sum_{a^\\prime \\in \\it{A}} \\pi(a^\\prime|s^\\prime)q_\\pi(s^\\prime,a^\\prime) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "### Bellman Optimal Equation\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/ddddfss.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/3334.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\\begin{align}\n",
    "v_*(s) &= \\underset{a}{\\operatorname{max}}  q_*(s,a) \\\\\n",
    "q_*(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_*(s^\\prime) \\\\\n",
    "v_*(s) &= \\underset{a}{\\operatorname{max}} \\Big( R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_*(s^\\prime) \\Big) \\\\\n",
    "q_*(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a \\underset{a^\\prime}{\\operatorname{max}}  q_*(s^\\prime,a^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Types of RL algorithms\n",
    "---\n",
    "- Value-based: estimate value function or Q-function of the optimal policy (no explicit policy)\n",
    "  > TD, Q-learning, DQN\n",
    "- Policy gradients: directly differentiate the above objective\n",
    "  > REINFORCE, TRPO\n",
    "- Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy\n",
    "  > Asynchronous advantage actor critic (A3C)\n",
    "- Model-based RL: estimate the transition model, and then use it for planning (no explicit policy) or to improve a policy\n",
    "  > Dyna, Guided Policy Search\n",
    "\n",
    "\n",
    "Policy Iteration\n",
    "---\n",
    "\\begin{align}\n",
    "v_{k+1}(s) &= \\sum_{a\\in \\it{A}} \\pi(a|s)\\Big(R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_k(s^\\prime)\\Big) \\\\\n",
    "\\pi^\\prime &= \\underset{a}{\\operatorname{argmax}}(v_\\pi) \\:(if\\:\\pi^\\prime \\geq \\pi)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/6d484ed095cba2cd7a8edf50b7e4e17e.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Value Iteration\n",
    "---\n",
    "\\begin{align}\n",
    "v_{k+1}(s) &= \\underset{a}{\\operatorname{max}} \\Big( R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_{k}(s^\\prime) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/fdfq3e.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic imports and setup\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import gym\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "def begin_grading(): print(\"\\x1b[43m\")\n",
    "def end_grading(): print(\"\\x1b[0m\")\n",
    "\n",
    "env = gym.make('FrozenLake-v0').unwrapped\n",
    "# Seed RNGs so you get the same printouts as me\n",
    "env.seed(0); from gym.spaces import prng; prng.seed(10)\n",
    "# Generate the episode\n",
    "env.reset()\n",
    "\n",
    "for t in range(100):\n",
    "    env.render()\n",
    "    a = env.action_space.sample()\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    if done:\n",
    "        break\n",
    "assert done\n",
    "env.render();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP(object):\n",
    "    def __init__(self, P, nS, nA, desc=None):\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "\n",
    "mdp = MDP( {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc)\n",
    "\n",
    "\n",
    "print(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "print(\"The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\")\n",
    "print(np.arange(16).reshape(4,4))\n",
    "print(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "print(\"For example, state 0 is the initial state, and the transition information for s=0, a=0 is \\nP[0][0] =\", mdp.P[0][0], \"\\n\")\n",
    "print(\"As another example, state 5 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\")\n",
    "print(\"P[5][0] =\", mdp.P[5][0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement value iteration\n",
    "In this problem, you'll implement value iteration, which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "For $i=0, 1, 2, \\dots$\n",
    "- $V^{(i+1)}(s) = \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "We additionally define the sequence of greedy policies $\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}$, where\n",
    "$$\\pi^{(i)}(s) = \\arg \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$$\n",
    "\n",
    "Your code will return two lists: $[V^{(0)}, V^{(1)}, \\dots, V^{(n)}]$ and $[\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}]$\n",
    "\n",
    "To ensure that you get the same policies as the reference solution, choose the lower-index action to break ties in $\\arg \\max_a$. This is done automatically by np.argmax. This will only affect the \"# chg actions\" printout below--it won't affect the values computed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, gamma, nIt):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mdp: MDP\n",
    "        gamma: discount factor\n",
    "        nIt: number of iterations, corresponding to n above\n",
    "    Outputs:\n",
    "        (value_functions, policies)\n",
    "        \n",
    "    len(value_functions) == nIt+1 and len(policies) == n\n",
    "    \"\"\"\n",
    "    print(\"Iteration | max|V-Vprev| | # chg actions | V[0]\")\n",
    "    print(\"----------+--------------+---------------+---------\")\n",
    "    Vs = [np.zeros(mdp.nS)] # list of value functions contains the initial value function V^{(0)}, which is zero\n",
    "    pis = []\n",
    "    for it in range(nIt):\n",
    "        oldpi = pis[-1] if len(pis) > 0 else None # \\pi^{(it)} = Greedy[V^{(it-1)}]. Just used for printout\n",
    "        Vprev = Vs[-1] # V^{(it)}\n",
    "        # YOUR CODE HERE\n",
    "        # Your code should define the following two variables\n",
    "        # pi: greedy policy for Vprev, \n",
    "        #     corresponding to the math above: \\pi^{(it)} = Greedy[V^{(it)}]\n",
    "        #     numpy array of ints\n",
    "        # V: bellman backup on Vprev\n",
    "        #     corresponding to the math above: V^{(it+1)} = T[V^{(it)}]\n",
    "        #     numpy array of floats\n",
    "        V = Vprev.copy()\n",
    "        pi = np.zeros(mdp.nS)\n",
    "        for i in range(mdp.nS):\n",
    "            max_Q = -np.inf\n",
    "            argmax_a = -1\n",
    "            for j in range(mdp.nA):\n",
    "                Q = 0\n",
    "                for pro, nxt_state, r in mdp.P[i][j]:\n",
    "                    Q += pro * (r + gamma * V[nxt_state])\n",
    "                if Q > max_Q:\n",
    "                    max_Q = Q\n",
    "                    argmax_a = j\n",
    "            V[i] = max_Q\n",
    "            pi[i] = argmax_a\n",
    "        ##################################################################3\n",
    "        \n",
    "        max_diff = np.abs(V - Vprev).max()\n",
    "        nChgActions=\"N/A\" if oldpi is None else (pi != oldpi).sum()\n",
    "        print(\"%4i      | %6.5f      | %4s          | %5.3f\"%(it, max_diff, nChgActions, V[0]))\n",
    "        Vs.append(V)\n",
    "        pis.append(pi)\n",
    "    return Vs, pis\n",
    "\n",
    "GAMMA=0.95 # we'll be using this same value in subsequent problems\n",
    "begin_grading()\n",
    "Vs_VI, pis_VI = value_iteration(mdp, gamma=GAMMA, nIt=20)\n",
    "end_grading()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for (V, pi) in zip(Vs_VI[:10], pis_VI[:10]):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(V.reshape(4,4), cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(4)-.5)\n",
    "    ax.set_yticks(np.arange(4)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {0: (-1, 0), 1:(0, -1), 2:(1,0), 3:(-1, 0)}\n",
    "    Pi = pi.reshape(4,4)\n",
    "    for y in range(4):\n",
    "        for x in range(4):\n",
    "            a = Pi[y, x]\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1) \n",
    "            plt.text(x, y, str(env.desc[y,x].item().decode()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "plt.figure()\n",
    "plt.plot(Vs_VI)\n",
    "plt.title(\"Values of different states\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/4225132.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Temporal Difference Learning\n",
    "---\n",
    "\\begin{align}\n",
    "v_{k+1}(s) &= \\underset{a}{\\operatorname{max}} \\Big( R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_{k}(s^\\prime) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "v(s_t) \\leftarrow v(s_t) + \\alpha(R_{t+1} + \\gamma v(s_{t+1}) - v(s_t)) \\\\\n",
    "R_{t+1} + \\gamma v(s_{t+1})\\::\\:TD\\:Target\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/TD2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### Model-Free Reinforcement Learning\n",
    "\\begin{align}\n",
    "\\pi^\\prime &= \\underset{a}{\\operatorname{argmax}} R_s^a + \\gamma v(s_{t+1}) \\\\\n",
    "           &= \\underset{a}{\\operatorname{argmax}} R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{s_ts_{t+1}}^a v(s_t)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/MC10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\\begin{align}\n",
    "q_{k+1}(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a \\underset{a^\\prime}{\\operatorname{max}}  q_k(s^\\prime,a^\\prime) \\\\\n",
    "q_{k+1}(s_t,a_t) & \\leftarrow q_k(s_t,a_t) + \\alpha(R_{t+1} + \\gamma q_k(s_{t+1},a_{t+1}) - q_k(s_t, a_t))\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/TD10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### $\\epsilon$-greedy\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/MC11.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### On-policy vs Off-policy\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/Off1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Q-learning\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/off6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/off7.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/off9.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Value Function Approximation (Non-tabular value function learning)\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx3.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx4.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx8.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx21.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/main-qimg-01c15f01cf6a56c19313c2791d5a9ae1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Deep Q Network\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx20.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/90-6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/Screen-Shot-2015-12-21-at-11.08.53-AM.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/dqn16.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/dqn17.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
