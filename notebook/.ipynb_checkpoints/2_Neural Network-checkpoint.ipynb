{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "---\n",
    "\n",
    "\\begin{align} y = f(w^T\\phi(x)) \\end{align}\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 \\end{align}\n",
    "\n",
    "<img src=\"https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/Single-Perceptron.png?x31195\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation with previous model\n",
    "---\n",
    "\n",
    "\\begin{align} y = f(w^T\\phi(x)) \\end{align}\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 \\end{align}\n",
    "\n",
    "\n",
    "### Perceptron Rule for SVM\n",
    "\n",
    "\\begin{align} y = sgn(w^{*T}x+b^*) \\end{align}\n",
    "\n",
    "\\begin{align} E_p(w) = -\\sum y_iw^Tx_i \\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/2012mdsp-pr13supportvectormachine-130701022429-phpapp02/95/2012mdsp-pr13-support-vector-machine-5-638.jpg?cb=1372645656\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function\n",
    "---\n",
    "\n",
    "### Probabilistic Supervised Learning - Logistic Regression\n",
    "\n",
    "![](http://www.saedsayad.com/images/LogReg_1.png)\n",
    "\n",
    "\\begin{align} p(y = 1 |x; \\theta) = \\sigma(\\theta^Tx) \\end{align}\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "\n",
    "### Information Theory\n",
    "\n",
    "\\begin{align} \\theta_{ML} = \\underset{\\theta}{\\operatorname{argmax}} P(Y|X; \\theta) \\end{align}\n",
    "\n",
    "- Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content what so ever.\n",
    "- Less likely events should have higher information content.\n",
    "- Independent events should have additive information.\n",
    "\n",
    "\\begin{align} I(X) = -logP(X) \\end{align}\n",
    "\n",
    "\n",
    "### Shannon's Entropy\n",
    "\n",
    "\\begin{align} H(X) = E_{x \\sim P}[I(X)] \\end{align}\n",
    "\n",
    "<img src=\"https://i.imgur.com/Pynf9sG.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "### Cross-Entropy\n",
    "\n",
    "데이터 분포 P(X) 상에서 추정 확률 분포 Q(X)가 가지는 정보의 기댓값\n",
    "\n",
    "\\begin{align} H(P,Q) = H(P) + D_{KL}(P||Q) \\end{align}\n",
    "\n",
    "\\begin{align} H(P,Q) = E_{x \\sim P}[\\log Q(X)] \\end{align}\n",
    "\n",
    "\n",
    "### Kullback-Leibler divergence, KLD\n",
    "\n",
    "두 확률분포의 차이를 계산하는 데 사용하는 함수. 가지고 있는 데이터의 분포 P(x)와 모델이 추정한 데이터의 분포 Q(x) 간의 차이\n",
    "\n",
    "\\begin{align} D_{KL}(P||Q) = E_{x \\sim P}[\\log\\frac{P(X)}{Q(X)}] \\end{align}\n",
    "\n",
    "\n",
    "### Relation between Cross Entropy Loss and Maximum Likelihood Estimation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization\n",
    "---\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/random-170910154045/95/-53-638.jpg?cb=1505089848\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "### Learning Theory - VC Dimension (Vapnik–Chervonenkis dimension)\n",
    "\n",
    "<img src=\"http://www.guochaoping.top/wp-content/uploads/2016/11/Sample_complexity_tradeoff.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\\begin{align} error_{test} \\geq error_{training} + \\sqrt{\\frac{1}{N}\\Big[D\\Big(\\log\\Big(\\frac{2N}{D}\\Big)+1\\Big)-\\log\\Big(\\frac{1}{\\delta}\\Big)\\Big]} \\end{align}\n",
    "\n",
    "\\begin{align} d_{VC} = \\frac{D}{N}, \\:\\: D:\\:model\\:complexity, \\:\\: N:\\:number\\:of\\:samples \\end{align}\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-03-at-7.52.01-PM-e1522832332857.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 + \\lambda ||\\theta||^2 \\end{align}\n",
    "\n",
    "\\begin{align} h_\\theta(x_i) = \\theta^Tx_i + b \\end{align}\n",
    "\n",
    "\\begin{align} \\theta^* = \\arg\\min_\\theta J(\\theta) \\end{align}\n",
    "\n",
    "\n",
    "### Drop Out\n",
    "\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "\n",
    "### Hyperparamter Training - Cross Validation\n",
    "\n",
    "<img src=\"http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/08_CV_Ensembling/fig-holdout.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/1fXzJ.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#MNIST Tutorials\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print('Using PyTorch version:', torch.__version__, 'CUDA:', cuda)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n",
    "\n",
    "pltsize=1\n",
    "plt.figure(figsize=(10*pltsize, pltsize))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1,10,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i,:,:,:].numpy().reshape(28,28), cmap=\"gray\")\n",
    "    plt.title('Class: '+str(y_train[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Techniques\n",
    "---\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "### Curriculum Learning\n",
    "\n",
    "### Supervised PreTraining"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
