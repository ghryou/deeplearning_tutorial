{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) tutorial - TTbot\n",
    "======================================\n",
    "\n",
    "<img src=\"https://www.naverlabs.com/naverlabs_/story/201803/1520480681892_TTbot_%ED%9D%B0%EB%B0%B0%EA%B2%BD.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTBot Simulator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simulator = {\"width\":31, \"height\":31, \"center\":15, \"resol\":3}\n",
    "map_param = {\"width\":100, \"height\":100, \"center\":50, \"resol\":1, \"scale\":5, \"Map.data.obstacle\": 200, \"Map.data.ball\": 1}\n",
    "# walls_samples = [[1.2,2.0],[1.4,2.0],[1.6,2.0],[1.2,4.0],[1.4,4.0],[1.6,4.0],[1.2,-2.0],[1.4,-2.0],[1.6,-2.0],[1.2,-4.0],[1.4,-4.0],[1.6,-4.0]]\n",
    "walls_samples = [[1.5,-30.0],[30.4,0.7],[-30.4,-0.7]]\n",
    "\n",
    "camera_fov = 120\n",
    "ball_blind_ratio = 1/np.tan(camera_fov/2*np.pi/180)\n",
    "ball_blind_bias = 5\n",
    "\n",
    "reward_region_x = 2\n",
    "reward_region_y = [0,1,2,3]\n",
    "\n",
    "trans_scale = int(simulator[\"resol\"]/map_param[\"resol\"])\n",
    "rot_scale = 20\n",
    "\n",
    "debug_scale = 10\n",
    "debug_scale_gray = 3\n",
    "\n",
    "max_iter = 99\n",
    "\n",
    "class TTbotGym:\n",
    "    def __init__(self, debug_flag=False, mlp_flag=False, test_flag=False, state_blink=True, state_inaccurate=True):\n",
    "        self.frame = np.zeros((simulator[\"height\"],simulator[\"width\"],1), np.uint8)\n",
    "        self.frame_gray = np.zeros((simulator[\"height\"]*debug_scale_gray,simulator[\"width\"]*debug_scale_gray,1), np.uint8)\n",
    "        self.balls = []\n",
    "        self.balls_prev = []\n",
    "        self.obstacles = []\n",
    "        self.episode_rewards = []\n",
    "        self.score = 0\n",
    "        self.iter = 0\n",
    "        self.done = False\n",
    "\n",
    "        self.write_flag = False\n",
    "        self.debug_flag = debug_flag\n",
    "        self.mlp_flag = mlp_flag\n",
    "        self.test_flag = test_flag\n",
    "        self.ball_inscreen_flag = 0\n",
    "        self.state_blink = state_blink\n",
    "        self.state_inaccurate = state_inaccurate\n",
    "\n",
    "        ############################################################\n",
    "        # Flag Setups\n",
    "        # debug_flag : Display colored map for demonstration\n",
    "        # mlp_flag : If true, step() returns small 31x31 gray scale image for Multi Layer Perception.\n",
    "        #            If false, step() returns 93x93 gray scale image for Convolutional Neural Network\n",
    "        # test_flag : Whether it is training phase or test phase. During the test phase, the simulator saves every demonstration in form of video\n",
    "        # write_flag : If true, it records videos\n",
    "        # state_blink : Simulate target's detection error\n",
    "        # state_inaccurate : Simulate target's position estimation error\n",
    "        ############################################################\n",
    "\n",
    "        ## DQN parameters\n",
    "        if self.mlp_flag:\n",
    "            self.observation_space = self.frame.copy()\n",
    "        else:\n",
    "            self.observation_space = self.frame_gray.copy()\n",
    "\n",
    "        self.action_space = np.array(range(10))\n",
    "\n",
    "\n",
    "        # self.reset(max_balls=20, max_walls=3)\n",
    "        return\n",
    "\n",
    "    def reset(self, max_balls=20, max_walls=2):\n",
    "        self.frame = np.zeros((simulator[\"height\"],simulator[\"width\"],1), np.uint8)\n",
    "        self.frame_gray = np.zeros((simulator[\"height\"]*debug_scale_gray,simulator[\"width\"]*debug_scale_gray,1), np.uint8)\n",
    "        self.balls = []\n",
    "        self.balls_prev = []\n",
    "        self.obstacles = []\n",
    "        self.score = 0\n",
    "        self.iter = 0\n",
    "        self.done = False\n",
    "        self.write_flag = False\n",
    "        self.ball_inscreen_flag = 0\n",
    "\n",
    "        if len(self.episode_rewards)%5000 == 0 and not self.test_flag:\n",
    "            self.write_flag = True\n",
    "            out_directory = \"data/video/tt.video.\"+format(len(self.episode_rewards)/5000,\"06\")+\".mp4\"\n",
    "\n",
    "        if self.test_flag:\n",
    "            self.write_flag = True\n",
    "            out_directory = \"data/video_test/tt.video.\"+format(len(self.episode_rewards),\"06\")+\".mp4\"\n",
    "\n",
    "        if self.write_flag:\n",
    "            codec = cv2.VideoWriter_fourcc(*'H264')\n",
    "            fps = 10\n",
    "            self.video = cv2.VideoWriter(out_directory, codec, fps, (simulator[\"width\"]*debug_scale,simulator[\"height\"]*debug_scale))\n",
    "\n",
    "        num_walls = int((max_walls+1)*random.random())\n",
    "        # walls_sampled = random.sample(walls_samples, num_walls)\n",
    "        rand_direction = random.random()\n",
    "        if rand_direction >= 0.666:\n",
    "            walls_sampled = [[random.random()+0.7,-10*random.random()-20],[-random.random()-0.2,-10*random.random()-20],\\\n",
    "                        [10*random.random()+20,0.5*random.random()+0.4],[-10*random.random()-20,-0.5*random.random()-0.4]]\n",
    "        elif rand_direction >= 0.333:\n",
    "            walls_sampled = [[random.random()+0.7,10*random.random()+20],[-random.random()-0.2,10*random.random()+20],\\\n",
    "                        [-10*random.random()-20,0.5*random.random()+0.4],[10*random.random()+20,-0.5*random.random()-0.4]]\n",
    "        else:\n",
    "            walls_sampled = []\n",
    "\n",
    "        obstacles_temp = []\n",
    "        for wall in walls_sampled:\n",
    "            if abs(wall[1]) >= abs(wall[0]):\n",
    "                point_start = -2.0*map_param[\"center\"]\n",
    "                point_end = 2.0*map_param[\"center\"]\n",
    "                unit = (point_end-point_start)/200\n",
    "\n",
    "                for i in range(200):\n",
    "                    cy = (point_start + unit*i)\n",
    "                    cx = (wall[0]*(map_param[\"center\"]-(cy/wall[1])))\n",
    "                    obstacles_temp.append([cx,cy])\n",
    "            else:\n",
    "                point_start = -1.0*map_param[\"center\"]\n",
    "                point_end = 3.0*map_param[\"center\"]\n",
    "                unit = (point_end-point_start)/200\n",
    "\n",
    "                for i in range(200):\n",
    "                    cx = (point_start + unit*i)\n",
    "                    cy = (wall[1]*(map_param[\"center\"]-(cx/wall[0])))\n",
    "                    obstacles_temp.append([cx,cy])\n",
    "\n",
    "        for obstacle in obstacles_temp:\n",
    "            cx = obstacle[0]\n",
    "            cy = obstacle[1]\n",
    "            insert = True\n",
    "            for wall in walls_sampled:\n",
    "                if cx/wall[0] + cy/wall[1] > map_param[\"center\"]:\n",
    "                    insert = False\n",
    "            if insert:\n",
    "                self.obstacles.append([cx,cy])\n",
    "\n",
    "        for i in range(max_balls):\n",
    "            cx = int(1.0*random.random()*(map_param[\"height\"]-2*trans_scale)+2*trans_scale)\n",
    "            cy = int(1.0*random.random()*map_param[\"width\"]) - map_param[\"center\"]\n",
    "            insert = True\n",
    "            for wall in walls_sampled:\n",
    "                if cx/wall[0] + cy/wall[1] >= map_param[\"center\"]:\n",
    "                    insert = False\n",
    "            if insert:\n",
    "                self.balls.append([cx,cy])\n",
    "\n",
    "        ## For Test - Put every ball on a single horizontal line\n",
    "        # for i in range(max_balls):\n",
    "        #     cx = int(0.3*(map_param[\"height\"]-2*trans_scale)+2*trans_scale)\n",
    "        #     cy = int(0.4*(int(1.0*i/max_balls*map_param[\"width\"]) - map_param[\"center\"]))\n",
    "        #     insert = True\n",
    "        #     for wall in walls_sampled:\n",
    "        #         if cx/wall[0] + cy/wall[1] >= map_param[\"center\"]:\n",
    "        #             insert = False\n",
    "        #     if insert:\n",
    "        #         self.balls.append([cx,cy])\n",
    "        #########################################################################################\n",
    "\n",
    "        self.frame, self.frame_gray = self.draw_state()\n",
    "\n",
    "        if self.mlp_flag:\n",
    "            return self.frame\n",
    "        else:\n",
    "            return self.frame_gray\n",
    "\n",
    "    def check_window_state(self, cx, cy):\n",
    "        inscreen = True\n",
    "        if cx < 0 or cx >= simulator[\"width\"]:\n",
    "            inscreen = False\n",
    "        if cy < 0 or cy >= simulator[\"height\"]:\n",
    "            inscreen = False\n",
    "        return inscreen\n",
    "\n",
    "    def draw_debug_frame(self, frame):\n",
    "        frame_debug = np.zeros((simulator[\"height\"]*debug_scale,simulator[\"width\"]*debug_scale,3), np.uint8)\n",
    "        for i in range(simulator[\"width\"]):\n",
    "            for j in range(simulator[\"height\"]):\n",
    "                if frame[i][j] == map_param[\"Map.data.obstacle\"]:\n",
    "                    cv2.rectangle(frame_debug,(i*debug_scale,j*debug_scale),((i+1)*debug_scale-1,(j+1)*debug_scale-1),(255,255,0),-1)\n",
    "                if frame[i][j] == map_param[\"Map.data.ball\"]:\n",
    "                    cv2.rectangle(frame_debug,(i*debug_scale,j*debug_scale),((i+1)*debug_scale-1,(j+1)*debug_scale-1),(0,255,0),-1)\n",
    "\n",
    "        cv2.rectangle(frame_debug,(simulator[\"center\"]*debug_scale-1,(simulator[\"height\"]-1)*debug_scale+1),\\\n",
    "                    ((simulator[\"center\"]+1)*debug_scale,simulator[\"height\"]*debug_scale-1),(255,0,0),-1)\n",
    "\n",
    "        for i in range(1,simulator[\"width\"]):\n",
    "            cv2.line(frame_debug,(i*debug_scale,0),(i*debug_scale,simulator[\"height\"]*debug_scale-1),(128,128,128),1)\n",
    "            cv2.line(frame_debug,(0,i*debug_scale),(simulator[\"width\"]*debug_scale-1,i*debug_scale),(128,128,128),1)\n",
    "\n",
    "        cv2.line(frame_debug,((simulator[\"center\"]+ball_blind_bias)*debug_scale,simulator[\"height\"]*debug_scale-1),\\\n",
    "                            (simulator[\"width\"]*debug_scale-1,(simulator[\"height\"]-int(ball_blind_ratio*(simulator[\"center\"]-1-ball_blind_bias)))*debug_scale),(128,128,128),1)\n",
    "        cv2.line(frame_debug,((simulator[\"center\"]-ball_blind_bias+1)*debug_scale,simulator[\"height\"]*debug_scale-1),\\\n",
    "                            (0,(simulator[\"height\"]-int(ball_blind_ratio*(simulator[\"center\"]-1-ball_blind_bias)))*debug_scale),(128,128,128),1)\n",
    "\n",
    "        cv2.rectangle(frame_debug,((simulator[\"center\"]-2)*debug_scale-1,(simulator[\"height\"]-2)*debug_scale+1),\\\n",
    "                    ((simulator[\"center\"]+3)*debug_scale,simulator[\"height\"]*debug_scale-1),(0,0,255),2)\n",
    "\n",
    "        cv2.putText(frame_debug,\"Score \"+str(self.score), (int(simulator[\"width\"]*debug_scale*0.65),int(simulator[\"width\"]*debug_scale*0.05)), cv2.FONT_HERSHEY_TRIPLEX, 0.5, (255,255,255))\n",
    "        cv2.putText(frame_debug,\"Step \"+str(self.iter), (int(simulator[\"width\"]*debug_scale*0.05),int(simulator[\"width\"]*debug_scale*0.05)), cv2.FONT_HERSHEY_TRIPLEX, 0.5, (255,255,255))\n",
    "\n",
    "        return frame_debug\n",
    "\n",
    "    def draw_state(self):\n",
    "        frame = np.zeros((simulator[\"height\"],simulator[\"width\"],1), np.uint8)\n",
    "        frame_gray = np.zeros((simulator[\"height\"]*debug_scale_gray,simulator[\"width\"]*debug_scale_gray,1), np.uint8)\n",
    "\n",
    "        for obstacle in self.obstacles:\n",
    "            cx = simulator[\"center\"] - int(round(1.0*obstacle[1]/trans_scale))\n",
    "            cy = simulator[\"height\"] - 1 - int(round(1.0*obstacle[0]/trans_scale))\n",
    "            if self.check_window_state(cx, cy):\n",
    "                frame[cx][cy] = map_param[\"Map.data.obstacle\"]\n",
    "        for ball in self.balls:\n",
    "            if self.state_blink == False or random.random() > (0.3 + ball[0]/3.0/map_param[\"center\"]):\n",
    "                if ball[0] >= int(ball_blind_ratio*(abs(1.0*ball[1])-ball_blind_bias)):\n",
    "                    ball_x = ball[0]\n",
    "                    ball_y = ball[1]\n",
    "                    if self.state_inaccurate:\n",
    "                        ball_x = ball_x + random.random()*map_param[\"center\"]*(0.1*ball_x*ball_x/map_param[\"center\"]/map_param[\"center\"] - 0.05)\n",
    "                        ball_y = ball_y + random.random()*map_param[\"center\"]*(0.1*ball_x*ball_x/map_param[\"center\"]/map_param[\"center\"] - 0.05)\n",
    "\n",
    "                    cx = simulator[\"center\"] - int(round(1.0*ball_y/trans_scale))\n",
    "                    cy = simulator[\"height\"] - 1 - int(round(1.0*ball_x/trans_scale))\n",
    "                    if self.check_window_state(cx, cy):\n",
    "                        frame[cx][cy] = map_param[\"Map.data.ball\"]\n",
    "        frame[simulator[\"center\"]][simulator[\"height\"]-1] = 255\n",
    "\n",
    "        if not self.mlp_flag:\n",
    "            gray_color = {\"ball\":255, \"wall\":100, \"robot\":200, \"robot_padding\":150}\n",
    "            for i in range(simulator[\"width\"]):\n",
    "                for j in range(simulator[\"height\"]):\n",
    "                    if frame[i][j] == map_param[\"Map.data.obstacle\"]:\n",
    "                        cv2.rectangle(frame_gray,(i*debug_scale_gray,j*debug_scale_gray),((i+1)*debug_scale_gray-1,(j+1)*debug_scale_gray-1),gray_color[\"wall\"],-1)\n",
    "                    if frame[i][j] == map_param[\"Map.data.ball\"]:\n",
    "                        cv2.rectangle(frame_gray,(i*debug_scale_gray,j*debug_scale_gray),((i+1)*debug_scale_gray-1,(j+1)*debug_scale_gray-1),gray_color[\"ball\"],-1)\n",
    "\n",
    "            cv2.rectangle(frame_gray,((simulator[\"center\"]-2)*debug_scale_gray-1,(simulator[\"height\"]-2)*debug_scale_gray+1),\\\n",
    "                        ((simulator[\"center\"]+3)*debug_scale_gray,simulator[\"height\"]*debug_scale_gray-1),gray_color[\"robot_padding\"],-1)\n",
    "            cv2.rectangle(frame_gray,(simulator[\"center\"]*debug_scale_gray-1,(simulator[\"height\"]-1)*debug_scale_gray+1),\\\n",
    "                        ((simulator[\"center\"]+1)*debug_scale_gray,simulator[\"height\"]*debug_scale_gray-1),gray_color[\"robot\"],-1)\n",
    "\n",
    "        return frame, frame_gray\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        reward = 0\n",
    "        balls_temp = []\n",
    "        for i, ball in enumerate(self.balls):\n",
    "            cx = int(round(1.0*ball[0]/trans_scale))\n",
    "            cy = int(round(abs(1.0*ball[1]/trans_scale)))\n",
    "            if  cx < reward_region_x and cx >= 0 and ball[0] >= int(ball_blind_ratio*(abs(1.0*ball[1])-ball_blind_bias)):\n",
    "                if cy <= reward_region_y[0]:\n",
    "                    reward = reward + 3\n",
    "                elif cy <= reward_region_y[1]:\n",
    "                    reward = reward + 2\n",
    "                elif cy <= reward_region_y[2]:\n",
    "                    reward = reward + 1\n",
    "                    if len(self.balls_prev) > 0:\n",
    "                        if int(round(1.0*self.balls_prev[i][0]/trans_scale)) < reward_region_x:\n",
    "                            reward = reward - 2\n",
    "                else:\n",
    "                    balls_temp.append(ball)\n",
    "            else:\n",
    "                balls_temp.append(ball)\n",
    "\n",
    "        balls_inscreen = []\n",
    "        for ball in balls_temp:\n",
    "            if ball[0] >= ball_blind_ratio * (abs(1.0*ball[1]) - ball_blind_bias)\\\n",
    "                and abs(1.0*ball[1]) <= map_param[\"center\"] and abs(1.0*ball[0]) < map_param[\"height\"]:\n",
    "                balls_inscreen.append(ball)\n",
    "\n",
    "        self.balls = balls_temp\n",
    "        if self.debug_flag:\n",
    "            print(\"balls length : \"+str(len(balls_temp))+\"  score : \"+str(self.score)+\"  screen_flag : \"+str(self.ball_inscreen_flag))\n",
    "\n",
    "        if action in range(10):\n",
    "            if len(balls_inscreen) == 0:\n",
    "                self.ball_inscreen_flag = self.ball_inscreen_flag + 1\n",
    "            else:\n",
    "                self.ball_inscreen_flag = 0\n",
    "\n",
    "        if len(balls_temp) == 0 or self.iter > max_iter or self.ball_inscreen_flag >= 10:\n",
    "            self.done = True\n",
    "\n",
    "        if self.done:\n",
    "            self.episode_rewards.append(self.score)\n",
    "            if self.write_flag:\n",
    "                self.video.release()\n",
    "                print(\"video saved\")\n",
    "\n",
    "        if action == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        if action in range(10):\n",
    "            self.iter = self.iter + 1\n",
    "\n",
    "        del_x, del_y, rot = 0, 0, 0\n",
    "\n",
    "        if action == 0: # forward\n",
    "            del_x, del_y = -1, 0\n",
    "        elif action == 1: # forward right\n",
    "            del_x, del_y = -1, 1\n",
    "        elif action == 2: # right\n",
    "            del_x, del_y = 0, 1\n",
    "        elif action == 3: # backward right\n",
    "            del_x, del_y = 1, 1\n",
    "        elif action == 4: # backward\n",
    "            del_x, del_y = 1, 0\n",
    "        elif action == 5: # bacward left\n",
    "            del_x, del_y = 1, -1\n",
    "        elif action == 6: # left\n",
    "            del_x, del_y = 0, -1\n",
    "        elif action == 7: # forward left\n",
    "            del_x, del_y = -1, -1\n",
    "        elif action == 8: # turn left\n",
    "            rot = -1\n",
    "        elif action == 9: # turn right\n",
    "            rot = 1\n",
    "        else:\n",
    "            del_x, del_y, rot_x = 0, 0, 0\n",
    "\n",
    "        balls_temp = []\n",
    "        obstacles_temp = []\n",
    "\n",
    "        del_x = del_x * trans_scale\n",
    "        del_y = del_y * trans_scale\n",
    "\n",
    "\n",
    "        # Update the positions of balls and obstacles\n",
    "        if len(self.balls) > 0:\n",
    "            balls_temp = np.add(self.balls, [del_x,del_y])\n",
    "\n",
    "        if len(self.obstacles) > 0:\n",
    "            obstacles_temp = np.add(self.obstacles, [del_x,del_y])\n",
    "\n",
    "        if action == 8 or action == 9:\n",
    "            if len(self.obstacles) > 0 and len(balls_temp) > 0:\n",
    "                points = np.concatenate((balls_temp, obstacles_temp))\n",
    "            else:\n",
    "                points = np.array(balls_temp)\n",
    "\n",
    "            if points.size > 0:\n",
    "                points = points.reshape(-1,2)\n",
    "                theta = rot_scale*rot*np.pi/180\n",
    "                theta_0 = np.arctan2(points.T[1],points.T[0])\n",
    "\n",
    "                ball_dist = np.linalg.norm(points, axis=1)\n",
    "                rot_delta_unit_x = np.subtract(np.cos(theta_0), np.cos(np.add(theta_0,theta)))\n",
    "                rot_delta_unit_y = np.subtract(np.sin(theta_0), np.sin(np.add(theta_0,theta)))\n",
    "                rot_delta_unit = np.concatenate((rot_delta_unit_x.reshape(-1,1),rot_delta_unit_y.reshape(-1,1)),axis=1)\n",
    "                ball_dist = np.concatenate((ball_dist.reshape(-1,1),ball_dist.reshape(-1,1)),axis=1)\n",
    "                rot_delta = np.multiply(ball_dist, rot_delta_unit)\n",
    "                points = np.subtract(points, rot_delta)\n",
    "\n",
    "                balls_temp = points[0:len(self.balls)]\n",
    "                obstacles_temp = points[len(self.balls):]\n",
    "\n",
    "        # Check collision\n",
    "        enable_move = True\n",
    "        for obstacle in obstacles_temp:\n",
    "            # if int(abs(1.0*obstacle[0]/trans_scale)) <= 0 and int(abs(1.0*obstacle[1]/trans_scale)) <= 0:\n",
    "            if abs(1.0*obstacle[0]) < 2.0 and abs(1.0*obstacle[1]) < 2.0:\n",
    "                enable_move = False\n",
    "\n",
    "        # Update observation state and calculate reward\n",
    "        reward = 0\n",
    "        if enable_move:\n",
    "            self.balls = balls_temp\n",
    "            reward = self.get_reward(action)\n",
    "            self.obstacles = obstacles_temp\n",
    "            self.frame, self.frame_gray = self.draw_state()\n",
    "            self.balls_prev = self.balls\n",
    "        else:\n",
    "            reward = self.get_reward(-1)\n",
    "\n",
    "        self.score = self.score + reward\n",
    "\n",
    "        # Draw debug frame to record video\n",
    "        if self.write_flag:\n",
    "            frame_debug = self.draw_debug_frame(self.frame)\n",
    "            self.video.write(frame_debug)\n",
    "\n",
    "        # Draw debug frame to display debug map\n",
    "        if self.debug_flag:\n",
    "            frame_debug = self.draw_debug_frame(self.frame)\n",
    "            if not is_ipython:\n",
    "                cv2.imshow(\"frame_debug\", frame_debug)\n",
    "                cv2.imshow(\"frame_debug_gray\", self.frame_gray)\n",
    "            else:\n",
    "                plt.imshow(cv2.cvtColor(frame_debug,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if self.mlp_flag:\n",
    "            return self.frame, reward, self.done\n",
    "        else:\n",
    "            return self.frame_gray, reward, self.done\n",
    "\n",
    "    def render(self):\n",
    "        frame_debug = self.draw_debug_frame(self.frame)\n",
    "        return frame_debug\n",
    "\n",
    "    def get_total_steps(self):\n",
    "        return self.iter\n",
    "\n",
    "    def get_episode_rewards(self):\n",
    "        return self.episode_rewards\n",
    "\n",
    "    def action_space_sample(self):\n",
    "        index = int(1.0*random.random()*10)\n",
    "        return self.action_space[index]\n",
    "\n",
    "\n",
    "env = TTbotGym(debug_flag=False, mlp_flag=False, test_flag=False, state_blink=True, state_inaccurate=True)\n",
    "env.reset()\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.imshow(cv2.cvtColor(env.render(),cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Replay Memory & DQN model & Epsilon-greedy function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TTbotGym(debug_flag=False, mlp_flag=False, test_flag=False, state_blink=True, state_inaccurate=True)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.linear1 = nn.Linear(2592,448)\n",
    "        self.linear2 = nn.Linear(448, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.linear1(x.view(x.size(0), -1)))\n",
    "        x = F.softmax(self.linear2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(10)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 running\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure(figsize = (7,7))\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "num_episodes = 50\n",
    "ep_monitor_rate = 10\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    ep_monitor_flag = False\n",
    "    if i_episode % ep_monitor_rate == 0:\n",
    "        ep_monitor_flag = True\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"episode %d running\"%(i_episode+1))\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state.reshape(-1,93,93)).unsqueeze(0).to(device).type('torch.FloatTensor')\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        \n",
    "        next_state = torch.from_numpy(next_state.reshape(-1,93,93)).unsqueeze(0).to(device).type('torch.FloatTensor')\n",
    "        reward = torch.tensor([reward], device=device).type('torch.FloatTensor')\n",
    "        \n",
    "        if ep_monitor_flag:\n",
    "            img.set_data(cv2.cvtColor(env.render(),cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if is_ipython:\n",
    "            if ep_monitor_flag:\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "        if not done:\n",
    "            next_state = state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            writer.add_scalar('data/TTbot/DQN/episode_durations', env.score, i_episode)\n",
    "            break\n",
    "\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.close()\n",
    "writer.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
