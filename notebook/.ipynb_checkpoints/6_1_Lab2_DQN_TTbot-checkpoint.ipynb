{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning (DQN) tutorial - TTbot\n",
    "======================================\n",
    "\n",
    "<img src=\"https://www.naverlabs.com/naverlabs_/story/201803/1520480681892_TTbot_%ED%9D%B0%EB%B0%B0%EA%B2%BD.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTBot Simulator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator = {\"width\":31, \"height\":31, \"center\":15, \"resol\":3}\n",
    "map_param = {\"width\":100, \"height\":100, \"center\":50, \"resol\":1, \"scale\":5, \"Map.data.obstacle\": 200, \"Map.data.ball\": 1}\n",
    "# walls_samples = [[1.2,2.0],[1.4,2.0],[1.6,2.0],[1.2,4.0],[1.4,4.0],[1.6,4.0],[1.2,-2.0],[1.4,-2.0],[1.6,-2.0],[1.2,-4.0],[1.4,-4.0],[1.6,-4.0]]\n",
    "walls_samples = [[1.5,-30.0],[30.4,0.7],[-30.4,-0.7]]\n",
    "\n",
    "camera_fov = 120\n",
    "ball_blind_ratio = 1/np.tan(camera_fov/2*np.pi/180)\n",
    "ball_blind_bias = 5\n",
    "\n",
    "reward_region_x = 2\n",
    "reward_region_y = [0,1,2,3]\n",
    "\n",
    "trans_scale = int(simulator[\"resol\"]/map_param[\"resol\"])\n",
    "rot_scale = 20\n",
    "\n",
    "debug_scale = 10\n",
    "debug_scale_gray = 3\n",
    "\n",
    "max_iter = 99\n",
    "\n",
    "class TTbotGym:\n",
    "    def __init__(self, debug_flag=False, mlp_flag=False, test_flag=False, state_blink=True, state_inaccurate=True):\n",
    "        self.frame = np.zeros((simulator[\"height\"],simulator[\"width\"],1), np.uint8)\n",
    "        self.frame_gray = np.zeros((simulator[\"height\"]*debug_scale_gray,simulator[\"width\"]*debug_scale_gray,1), np.uint8)\n",
    "        self.balls = []\n",
    "        self.balls_prev = []\n",
    "        self.obstacles = []\n",
    "        self.episode_rewards = []\n",
    "        self.score = 0\n",
    "        self.iter = 0\n",
    "        self.done = False\n",
    "\n",
    "        self.write_flag = False\n",
    "        self.debug_flag = debug_flag\n",
    "        self.mlp_flag = mlp_flag\n",
    "        self.test_flag = test_flag\n",
    "        self.ball_inscreen_flag = 0\n",
    "        self.state_blink = state_blink\n",
    "        self.state_inaccurate = state_inaccurate\n",
    "\n",
    "        ############################################################\n",
    "        # Flag Setups\n",
    "        # debug_flag : Display colored map for demonstration\n",
    "        # mlp_flag : If true, step() returns small 31x31 gray scale image for Multi Layer Perception.\n",
    "        #            If false, step() returns 93x93 gray scale image for Convolutional Neural Network\n",
    "        # test_flag : Whether it is training phase or test phase. During the test phase, the simulator saves every demonstration in form of video\n",
    "        # write_flag : If true, it records videos\n",
    "        # state_blink : Simulate target's detection error\n",
    "        # state_inaccurate : Simulate target's position estimation error\n",
    "        ############################################################\n",
    "\n",
    "        ## DQN parameters\n",
    "        if self.mlp_flag:\n",
    "            self.observation_space = self.frame.copy()\n",
    "        else:\n",
    "            self.observation_space = self.frame_gray.copy()\n",
    "\n",
    "        self.action_space = np.array(range(10))\n",
    "\n",
    "\n",
    "        # self.reset(max_balls=20, max_walls=3)\n",
    "        return\n",
    "\n",
    "    def reset(self, max_balls=20, max_walls=2):\n",
    "        self.frame = np.zeros((simulator[\"height\"],simulator[\"width\"],1), np.uint8)\n",
    "        self.frame_gray = np.zeros((simulator[\"height\"]*debug_scale_gray,simulator[\"width\"]*debug_scale_gray,1), np.uint8)\n",
    "        self.balls = []\n",
    "        self.balls_prev = []\n",
    "        self.obstacles = []\n",
    "        self.score = 0\n",
    "        self.iter = 0\n",
    "        self.done = False\n",
    "        self.write_flag = False\n",
    "        self.ball_inscreen_flag = 0\n",
    "\n",
    "        if len(self.episode_rewards)%5000 == 0 and not self.test_flag:\n",
    "            self.write_flag = True\n",
    "            out_directory = \"data/video/tt.video.\"+format(len(self.episode_rewards)/5000,\"06\")+\".mp4\"\n",
    "\n",
    "        if self.test_flag:\n",
    "            self.write_flag = True\n",
    "            out_directory = \"data/video_test/tt.video.\"+format(len(self.episode_rewards),\"06\")+\".mp4\"\n",
    "\n",
    "        if self.write_flag:\n",
    "            codec = cv2.VideoWriter_fourcc(*'H264')\n",
    "            fps = 10\n",
    "            self.video = cv2.VideoWriter(out_directory, codec, fps, (simulator[\"width\"]*debug_scale,simulator[\"height\"]*debug_scale))\n",
    "\n",
    "        num_walls = int((max_walls+1)*random.random())\n",
    "        # walls_sampled = random.sample(walls_samples, num_walls)\n",
    "        rand_direction = random.random()\n",
    "        if rand_direction >= 0.666:\n",
    "            walls_sampled = [[random.random()+0.7,-10*random.random()-20],[-random.random()-0.2,-10*random.random()-20],\\\n",
    "                        [10*random.random()+20,0.5*random.random()+0.4],[-10*random.random()-20,-0.5*random.random()-0.4]]\n",
    "        elif rand_direction >= 0.333:\n",
    "            walls_sampled = [[random.random()+0.7,10*random.random()+20],[-random.random()-0.2,10*random.random()+20],\\\n",
    "                        [-10*random.random()-20,0.5*random.random()+0.4],[10*random.random()+20,-0.5*random.random()-0.4]]\n",
    "        else:\n",
    "            walls_sampled = []\n",
    "\n",
    "        obstacles_temp = []\n",
    "        for wall in walls_sampled:\n",
    "            if abs(wall[1]) >= abs(wall[0]):\n",
    "                point_start = -2.0*map_param[\"center\"]\n",
    "                point_end = 2.0*map_param[\"center\"]\n",
    "                unit = (point_end-point_start)/200\n",
    "\n",
    "                for i in range(200):\n",
    "                    cy = (point_start + unit*i)\n",
    "                    cx = (wall[0]*(map_param[\"center\"]-(cy/wall[1])))\n",
    "                    obstacles_temp.append([cx,cy])\n",
    "            else:\n",
    "                point_start = -1.0*map_param[\"center\"]\n",
    "                point_end = 3.0*map_param[\"center\"]\n",
    "                unit = (point_end-point_start)/200\n",
    "\n",
    "                for i in range(200):\n",
    "                    cx = (point_start + unit*i)\n",
    "                    cy = (wall[1]*(map_param[\"center\"]-(cx/wall[0])))\n",
    "                    obstacles_temp.append([cx,cy])\n",
    "\n",
    "        for obstacle in obstacles_temp:\n",
    "            cx = obstacle[0]\n",
    "            cy = obstacle[1]\n",
    "            insert = True\n",
    "            for wall in walls_sampled:\n",
    "                if cx/wall[0] + cy/wall[1] > map_param[\"center\"]:\n",
    "                    insert = False\n",
    "            if insert:\n",
    "                self.obstacles.append([cx,cy])\n",
    "\n",
    "        for i in range(max_balls):\n",
    "            cx = int(1.0*random.random()*(map_param[\"height\"]-2*trans_scale)+2*trans_scale)\n",
    "            cy = int(1.0*random.random()*map_param[\"width\"]) - map_param[\"center\"]\n",
    "            insert = True\n",
    "            for wall in walls_sampled:\n",
    "                if cx/wall[0] + cy/wall[1] >= map_param[\"center\"]:\n",
    "                    insert = False\n",
    "            if insert:\n",
    "                self.balls.append([cx,cy])\n",
    "\n",
    "        ## For Test - Put every ball on a single horizontal line\n",
    "        # for i in range(max_balls):\n",
    "        #     cx = int(0.3*(map_param[\"height\"]-2*trans_scale)+2*trans_scale)\n",
    "        #     cy = int(0.4*(int(1.0*i/max_balls*map_param[\"width\"]) - map_param[\"center\"]))\n",
    "        #     insert = True\n",
    "        #     for wall in walls_sampled:\n",
    "        #         if cx/wall[0] + cy/wall[1] >= map_param[\"center\"]:\n",
    "        #             insert = False\n",
    "        #     if insert:\n",
    "        #         self.balls.append([cx,cy])\n",
    "        #########################################################################################\n",
    "\n",
    "        self.frame, self.frame_gray = self.draw_state()\n",
    "\n",
    "        if self.mlp_flag:\n",
    "            return self.frame\n",
    "        else:\n",
    "            return self.frame_gray\n",
    "\n",
    "    def check_window_state(self, cx, cy):\n",
    "        inscreen = True\n",
    "        if cx < 0 or cx >= simulator[\"width\"]:\n",
    "            inscreen = False\n",
    "        if cy < 0 or cy >= simulator[\"height\"]:\n",
    "            inscreen = False\n",
    "        return inscreen\n",
    "\n",
    "    def draw_debug_frame(self, frame):\n",
    "        frame_debug = np.zeros((simulator[\"height\"]*debug_scale,simulator[\"width\"]*debug_scale,3), np.uint8)\n",
    "        for i in range(simulator[\"width\"]):\n",
    "            for j in range(simulator[\"height\"]):\n",
    "                if frame[i][j] == map_param[\"Map.data.obstacle\"]:\n",
    "                    cv2.rectangle(frame_debug,(i*debug_scale,j*debug_scale),((i+1)*debug_scale-1,(j+1)*debug_scale-1),(255,255,0),-1)\n",
    "                if frame[i][j] == map_param[\"Map.data.ball\"]:\n",
    "                    cv2.rectangle(frame_debug,(i*debug_scale,j*debug_scale),((i+1)*debug_scale-1,(j+1)*debug_scale-1),(0,255,0),-1)\n",
    "\n",
    "        cv2.rectangle(frame_debug,(simulator[\"center\"]*debug_scale-1,(simulator[\"height\"]-1)*debug_scale+1),\\\n",
    "                    ((simulator[\"center\"]+1)*debug_scale,simulator[\"height\"]*debug_scale-1),(255,0,0),-1)\n",
    "\n",
    "        for i in range(1,simulator[\"width\"]):\n",
    "            cv2.line(frame_debug,(i*debug_scale,0),(i*debug_scale,simulator[\"height\"]*debug_scale-1),(128,128,128),1)\n",
    "            cv2.line(frame_debug,(0,i*debug_scale),(simulator[\"width\"]*debug_scale-1,i*debug_scale),(128,128,128),1)\n",
    "\n",
    "        cv2.line(frame_debug,((simulator[\"center\"]+ball_blind_bias)*debug_scale,simulator[\"height\"]*debug_scale-1),\\\n",
    "                            (simulator[\"width\"]*debug_scale-1,(simulator[\"height\"]-int(ball_blind_ratio*(simulator[\"center\"]-1-ball_blind_bias)))*debug_scale),(128,128,128),1)\n",
    "        cv2.line(frame_debug,((simulator[\"center\"]-ball_blind_bias+1)*debug_scale,simulator[\"height\"]*debug_scale-1),\\\n",
    "                            (0,(simulator[\"height\"]-int(ball_blind_ratio*(simulator[\"center\"]-1-ball_blind_bias)))*debug_scale),(128,128,128),1)\n",
    "\n",
    "        cv2.rectangle(frame_debug,((simulator[\"center\"]-2)*debug_scale-1,(simulator[\"height\"]-2)*debug_scale+1),\\\n",
    "                    ((simulator[\"center\"]+3)*debug_scale,simulator[\"height\"]*debug_scale-1),(0,0,255),2)\n",
    "\n",
    "        cv2.putText(frame_debug,\"Score \"+str(self.score), (int(simulator[\"width\"]*debug_scale*0.65),int(simulator[\"width\"]*debug_scale*0.05)), cv2.FONT_HERSHEY_TRIPLEX, 0.5, (255,255,255))\n",
    "        cv2.putText(frame_debug,\"Step \"+str(self.iter), (int(simulator[\"width\"]*debug_scale*0.05),int(simulator[\"width\"]*debug_scale*0.05)), cv2.FONT_HERSHEY_TRIPLEX, 0.5, (255,255,255))\n",
    "\n",
    "        return frame_debug\n",
    "\n",
    "    def draw_state(self):\n",
    "        frame = np.zeros((simulator[\"height\"],simulator[\"width\"],1), np.uint8)\n",
    "        frame_gray = np.zeros((simulator[\"height\"]*debug_scale_gray,simulator[\"width\"]*debug_scale_gray,1), np.uint8)\n",
    "\n",
    "        for obstacle in self.obstacles:\n",
    "            cx = simulator[\"center\"] - int(round(1.0*obstacle[1]/trans_scale))\n",
    "            cy = simulator[\"height\"] - 1 - int(round(1.0*obstacle[0]/trans_scale))\n",
    "            if self.check_window_state(cx, cy):\n",
    "                frame[cx][cy] = map_param[\"Map.data.obstacle\"]\n",
    "        for ball in self.balls:\n",
    "            if self.state_blink == False or random.random() > (0.3 + ball[0]/3.0/map_param[\"center\"]):\n",
    "                if ball[0] >= int(ball_blind_ratio*(abs(1.0*ball[1])-ball_blind_bias)):\n",
    "                    ball_x = ball[0]\n",
    "                    ball_y = ball[1]\n",
    "                    if self.state_inaccurate:\n",
    "                        ball_x = ball_x + random.random()*map_param[\"center\"]*(0.1*ball_x*ball_x/map_param[\"center\"]/map_param[\"center\"] - 0.05)\n",
    "                        ball_y = ball_y + random.random()*map_param[\"center\"]*(0.1*ball_x*ball_x/map_param[\"center\"]/map_param[\"center\"] - 0.05)\n",
    "\n",
    "                    cx = simulator[\"center\"] - int(round(1.0*ball_y/trans_scale))\n",
    "                    cy = simulator[\"height\"] - 1 - int(round(1.0*ball_x/trans_scale))\n",
    "                    if self.check_window_state(cx, cy):\n",
    "                        frame[cx][cy] = map_param[\"Map.data.ball\"]\n",
    "        frame[simulator[\"center\"]][simulator[\"height\"]-1] = 255\n",
    "\n",
    "        if not self.mlp_flag:\n",
    "            gray_color = {\"ball\":255, \"wall\":100, \"robot\":200, \"robot_padding\":150}\n",
    "            for i in range(simulator[\"width\"]):\n",
    "                for j in range(simulator[\"height\"]):\n",
    "                    if frame[i][j] == map_param[\"Map.data.obstacle\"]:\n",
    "                        cv2.rectangle(frame_gray,(i*debug_scale_gray,j*debug_scale_gray),((i+1)*debug_scale_gray-1,(j+1)*debug_scale_gray-1),gray_color[\"wall\"],-1)\n",
    "                    if frame[i][j] == map_param[\"Map.data.ball\"]:\n",
    "                        cv2.rectangle(frame_gray,(i*debug_scale_gray,j*debug_scale_gray),((i+1)*debug_scale_gray-1,(j+1)*debug_scale_gray-1),gray_color[\"ball\"],-1)\n",
    "\n",
    "            cv2.rectangle(frame_gray,((simulator[\"center\"]-2)*debug_scale_gray-1,(simulator[\"height\"]-2)*debug_scale_gray+1),\\\n",
    "                        ((simulator[\"center\"]+3)*debug_scale_gray,simulator[\"height\"]*debug_scale_gray-1),gray_color[\"robot_padding\"],-1)\n",
    "            cv2.rectangle(frame_gray,(simulator[\"center\"]*debug_scale_gray-1,(simulator[\"height\"]-1)*debug_scale_gray+1),\\\n",
    "                        ((simulator[\"center\"]+1)*debug_scale_gray,simulator[\"height\"]*debug_scale_gray-1),gray_color[\"robot\"],-1)\n",
    "\n",
    "        return frame, frame_gray\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        reward = 0\n",
    "        balls_temp = []\n",
    "        for i, ball in enumerate(self.balls):\n",
    "            cx = int(round(1.0*ball[0]/trans_scale))\n",
    "            cy = int(round(abs(1.0*ball[1]/trans_scale)))\n",
    "            if  cx < reward_region_x and cx >= 0 and ball[0] >= int(ball_blind_ratio*(abs(1.0*ball[1])-ball_blind_bias)):\n",
    "                if cy <= reward_region_y[0]:\n",
    "                    reward = reward + 3\n",
    "                elif cy <= reward_region_y[1]:\n",
    "                    reward = reward + 2\n",
    "                elif cy <= reward_region_y[2]:\n",
    "                    reward = reward + 1\n",
    "                    if len(self.balls_prev) > 0:\n",
    "                        if int(round(1.0*self.balls_prev[i][0]/trans_scale)) < reward_region_x:\n",
    "                            reward = reward - 2\n",
    "                else:\n",
    "                    balls_temp.append(ball)\n",
    "            else:\n",
    "                balls_temp.append(ball)\n",
    "\n",
    "        balls_inscreen = []\n",
    "        for ball in balls_temp:\n",
    "            if ball[0] >= ball_blind_ratio * (abs(1.0*ball[1]) - ball_blind_bias)\\\n",
    "                and abs(1.0*ball[1]) <= map_param[\"center\"] and abs(1.0*ball[0]) < map_param[\"height\"]:\n",
    "                balls_inscreen.append(ball)\n",
    "\n",
    "        self.balls = balls_temp\n",
    "        if self.debug_flag:\n",
    "            print(\"balls length : \"+str(len(balls_temp))+\"  score : \"+str(self.score)+\"  screen_flag : \"+str(self.ball_inscreen_flag))\n",
    "\n",
    "        if action in range(10):\n",
    "            if len(balls_inscreen) == 0:\n",
    "                self.ball_inscreen_flag = self.ball_inscreen_flag + 1\n",
    "            else:\n",
    "                self.ball_inscreen_flag = 0\n",
    "\n",
    "        if len(balls_temp) == 0 or self.iter > max_iter or self.ball_inscreen_flag >= 10:\n",
    "            self.done = True\n",
    "\n",
    "        if self.done:\n",
    "            self.episode_rewards.append(self.score)\n",
    "            if self.write_flag:\n",
    "                self.video.release()\n",
    "                print(\"video saved\")\n",
    "\n",
    "        if action == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        if action in range(10):\n",
    "            self.iter = self.iter + 1\n",
    "\n",
    "        del_x, del_y, rot = 0, 0, 0\n",
    "\n",
    "        if action == 0: # forward\n",
    "            del_x, del_y = -1, 0\n",
    "        elif action == 1: # forward right\n",
    "            del_x, del_y = -1, 1\n",
    "        elif action == 2: # right\n",
    "            del_x, del_y = 0, 1\n",
    "        elif action == 3: # backward right\n",
    "            del_x, del_y = 1, 1\n",
    "        elif action == 4: # backward\n",
    "            del_x, del_y = 1, 0\n",
    "        elif action == 5: # bacward left\n",
    "            del_x, del_y = 1, -1\n",
    "        elif action == 6: # left\n",
    "            del_x, del_y = 0, -1\n",
    "        elif action == 7: # forward left\n",
    "            del_x, del_y = -1, -1\n",
    "        elif action == 8: # turn left\n",
    "            rot = -1\n",
    "        elif action == 9: # turn right\n",
    "            rot = 1\n",
    "        else:\n",
    "            del_x, del_y, rot_x = 0, 0, 0\n",
    "\n",
    "        balls_temp = []\n",
    "        obstacles_temp = []\n",
    "\n",
    "        del_x = del_x * trans_scale\n",
    "        del_y = del_y * trans_scale\n",
    "\n",
    "\n",
    "        # Update the positions of balls and obstacles\n",
    "        if len(self.balls) > 0:\n",
    "            balls_temp = np.add(self.balls, [del_x,del_y])\n",
    "\n",
    "        if len(self.obstacles) > 0:\n",
    "            obstacles_temp = np.add(self.obstacles, [del_x,del_y])\n",
    "\n",
    "        if action == 8 or action == 9:\n",
    "            if len(self.obstacles) > 0 and len(balls_temp) > 0:\n",
    "                points = np.concatenate((balls_temp, obstacles_temp))\n",
    "            else:\n",
    "                points = np.array(balls_temp)\n",
    "\n",
    "            if points.size > 0:\n",
    "                points = points.reshape(-1,2)\n",
    "                theta = rot_scale*rot*np.pi/180\n",
    "                theta_0 = np.arctan2(points.T[1],points.T[0])\n",
    "\n",
    "                ball_dist = np.linalg.norm(points, axis=1)\n",
    "                rot_delta_unit_x = np.subtract(np.cos(theta_0), np.cos(np.add(theta_0,theta)))\n",
    "                rot_delta_unit_y = np.subtract(np.sin(theta_0), np.sin(np.add(theta_0,theta)))\n",
    "                rot_delta_unit = np.concatenate((rot_delta_unit_x.reshape(-1,1),rot_delta_unit_y.reshape(-1,1)),axis=1)\n",
    "                ball_dist = np.concatenate((ball_dist.reshape(-1,1),ball_dist.reshape(-1,1)),axis=1)\n",
    "                rot_delta = np.multiply(ball_dist, rot_delta_unit)\n",
    "                points = np.subtract(points, rot_delta)\n",
    "\n",
    "                balls_temp = points[0:len(self.balls)]\n",
    "                obstacles_temp = points[len(self.balls):]\n",
    "\n",
    "        # Check collision\n",
    "        enable_move = True\n",
    "        for obstacle in obstacles_temp:\n",
    "            # if int(abs(1.0*obstacle[0]/trans_scale)) <= 0 and int(abs(1.0*obstacle[1]/trans_scale)) <= 0:\n",
    "            if abs(1.0*obstacle[0]) < 2.0 and abs(1.0*obstacle[1]) < 2.0:\n",
    "                enable_move = False\n",
    "\n",
    "        # Update observation state and calculate reward\n",
    "        reward = 0\n",
    "        if enable_move:\n",
    "            self.balls = balls_temp\n",
    "            reward = self.get_reward(action)\n",
    "            self.obstacles = obstacles_temp\n",
    "            self.frame, self.frame_gray = self.draw_state()\n",
    "            self.balls_prev = self.balls\n",
    "        else:\n",
    "            reward = self.get_reward(-1)\n",
    "\n",
    "        self.score = self.score + reward\n",
    "\n",
    "        # Draw debug frame to record video\n",
    "        if self.write_flag:\n",
    "            frame_debug = self.draw_debug_frame(self.frame)\n",
    "            self.video.write(frame_debug)\n",
    "\n",
    "        # Draw debug frame to display debug map\n",
    "        if self.debug_flag:\n",
    "            frame_debug = self.draw_debug_frame(self.frame)\n",
    "            if not is_ipython:\n",
    "                cv2.imshow(\"frame_debug\", frame_debug)\n",
    "                cv2.imshow(\"frame_debug_gray\", self.frame_gray)\n",
    "            else:\n",
    "                plt.imshow(cv2.cvtColor(frame_debug,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if self.mlp_flag:\n",
    "            return self.frame, reward, self.done\n",
    "        else:\n",
    "            return self.frame_gray, reward, self.done\n",
    "\n",
    "    def render(self):\n",
    "        frame_debug = self.draw_debug_frame(self.frame)\n",
    "        return frame_debug\n",
    "\n",
    "    def get_total_steps(self):\n",
    "        return self.iter\n",
    "\n",
    "    def get_episode_rewards(self):\n",
    "        return self.episode_rewards\n",
    "\n",
    "    def action_space_sample(self):\n",
    "        index = int(1.0*random.random()*10)\n",
    "        return self.action_space[index]\n",
    "\n",
    "\n",
    "env = TTbotGym(debug_flag=False, mlp_flag=False, test_flag=False, state_blink=True, state_inaccurate=True)\n",
    "env.reset()\n",
    "plt.figure(figsize = (7,7))\n",
    "plt.imshow(cv2.cvtColor(env.render(),cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Replay Memory & DQN model & Epsilon-greedy function\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TTbotGym(debug_flag=False, mlp_flag=False, test_flag=False, state_blink=True, state_inaccurate=True)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.linear1 = nn.Linear(2592,448)\n",
    "        self.linear2 = nn.Linear(448, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.linear1(x.view(x.size(0), -1)))\n",
    "        x = F.softmax(self.linear2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(1000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(10)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "plt.figure(figsize = (7,7))\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "num_episodes = 50\n",
    "ep_monitor_rate = 10\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    ep_monitor_flag = False\n",
    "    if i_episode % ep_monitor_rate == 0:\n",
    "        ep_monitor_flag = True\n",
    "\n",
    "    # Initialize the environment and state\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"episode %d running\"%(i_episode+1))\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state.reshape(-1,93,93)).unsqueeze(0).to(device).type('torch.FloatTensor')\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        \n",
    "        next_state = torch.from_numpy(next_state.reshape(-1,93,93)).unsqueeze(0).to(device).type('torch.FloatTensor')\n",
    "        reward = torch.tensor([reward], device=device).type('torch.FloatTensor')\n",
    "        \n",
    "        if ep_monitor_flag:\n",
    "            img.set_data(cv2.cvtColor(env.render(),cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        if is_ipython:\n",
    "            if ep_monitor_flag:\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "        if not done:\n",
    "            next_state = state\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            writer.add_scalar('data/TTbot/DQN/episode_durations', env.score, i_episode)\n",
    "            break\n",
    "\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.close()\n",
    "writer.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
