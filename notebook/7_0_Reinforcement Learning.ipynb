{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Berkeley CS188](http://ai.berkeley.edu/lecture_slides.html)\n",
    "\n",
    "[Berkeley CS294](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/index.html)\n",
    "\n",
    "[Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)\n",
    "\n",
    "\n",
    "MDP (Markov Decision Process)\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/9864ef6a012bcbff9249a3805b06035d.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/b256481449d77879cff9109fbecb08d1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Value Function\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/2f32323a0ff14183c045cfb04744ab73.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### State - Value Function\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/4885d4877f3115bb054016dbd00e14ea.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Action - Value Function (Q Function)\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/e7b067d294a64c295cd120d1cdf33e20.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### Bellman Equation\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/dfq.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/18eba72dcfeafa6e6280055a95078ffa.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Bellman Expectation Equation\n",
    "\\begin{align}\n",
    "v_\\pi(s) &= \\sum_{a\\in \\it{A}} \\pi(a|s)q_\\pi(s,a) \\\\\n",
    "q_\\pi(s,a) &= R(s,a) + \\gamma \\mathbb{E}_\\pi[q_\\pi(S_{t+1}, A_{t+1} | S_t=s, A_t=a)] \\\\\n",
    "       &= R(s,a) + \\gamma \\sum_{s^\\prime\\in \\it{S}} \\mathbb{P}(S_{t+1} = s^\\prime|S_t = s, A_t = a) \\Big( \\sum_{a^\\prime \\in \\it{A}} \\pi(a^\\prime|s^\\prime)q_\\pi(s^\\prime,a^\\prime) \\Big) \\\\\n",
    "       &= R(s,a) + \\gamma \\sum_{s^\\prime\\in \\it{S}} \\mathbb{P}(S_{t+1} = s^\\prime|S_t = s, A_t = a)v_\\pi(s) \\\\\n",
    "       &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_\\pi(s^\\prime) \\\\\n",
    "v_\\pi(s) &= \\sum_{a\\in \\it{A}} \\pi(a|s)\\Big(R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_\\pi(s^\\prime)\\Big) \\\\\n",
    "q_\\pi(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a \\Big( \\sum_{a^\\prime \\in \\it{A}} \\pi(a^\\prime|s^\\prime)q_\\pi(s^\\prime,a^\\prime) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "### Bellman Optimal Equation\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/ddddfss.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/3334.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\\begin{align}\n",
    "v_*(s) &= \\underset{a}{\\operatorname{max}}  q_*(s,a) \\\\\n",
    "q_*(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_*(s^\\prime) \\\\\n",
    "v_*(s) &= \\underset{a}{\\operatorname{max}} \\Big( R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_*(s^\\prime) \\Big) \\\\\n",
    "q_*(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a \\underset{a^\\prime}{\\operatorname{max}}  q_*(s^\\prime,a^\\prime)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Types of RL algorithms\n",
    "---\n",
    "- Value-based: estimate value function or Q-function of the optimal policy (no explicit policy)\n",
    "  > TD, Q-learning, DQN\n",
    "- Policy gradients: directly differentiate the above objective\n",
    "  > REINFORCE, TRPO\n",
    "- Actor-critic: estimate value function or Q-function of the current policy, use it to improve policy\n",
    "  > Asynchronous advantage actor critic (A3C)\n",
    "- Model-based RL: estimate the transition model, and then use it for planning (no explicit policy) or to improve a policy\n",
    "  > Dyna, Guided Policy Search\n",
    "\n",
    "\n",
    "Value Iteration\n",
    "---\n",
    "\\begin{align}\n",
    "v_{k+1}(s) &= \\underset{a}{\\operatorname{max}} \\Big( R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_{k}(s^\\prime) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/fdfq3e.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Policy Iteration\n",
    "---\n",
    "\\begin{align}\n",
    "v_{k+1}(s) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_k(s^\\prime) \\\\\n",
    "\\pi^\\prime &= \\underset{a}{\\operatorname{argmax}}\\Big(R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_k(s^\\prime)\\Big) \\:(if\\:\\pi^\\prime \\geq \\pi) \\\\\n",
    "\\pi^\\prime &= \\underset{a}{\\operatorname{argmax}}\\Big(Q(s,a)\\Big)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/6d484ed095cba2cd7a8edf50b7e4e17e.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"../res/Screenshot from 2018-07-30 09-35-36.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### No Discount, No Noise\n",
    "\n",
    "<img src=\"../res/ch7_rl_value_iteration_0.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"../res/ch7_rl_policy_iteration_0.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### No Discount, Noise 0.2\n",
    "\n",
    "<img src=\"../res/ch7_rl_value_iteration_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"../res/ch7_rl_policy_iteration_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "#### Discount 0.9, Noise 0.2\n",
    "\n",
    "<img src=\"../res/ch7_rl_value_iteration_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "<img src=\"../res/ch7_rl_policy_iteration_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/4225132.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Temporal Difference Learning\n",
    "---\n",
    "\\begin{align}\n",
    "v_{k+1}(s) &= \\underset{a}{\\operatorname{max}} \\Big( R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a v_{k}(s^\\prime) \\Big)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "v(s_t) \\leftarrow (1-\\alpha)v(s_t) + \\alpha(R_{t+1} + \\gamma v(s_{t+1})) \\\\\n",
    "R_{t+1} + \\gamma v(s_{t+1})\\::\\:TD\\:Target\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/TD2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### Model-Free Reinforcement Learning\n",
    "\\begin{align}\n",
    "\\pi^\\prime &= \\underset{a}{\\operatorname{argmax}} R_s^a + \\gamma v(s_{t+1}) \\\\\n",
    "           &= \\underset{a}{\\operatorname{argmax}} R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{s_ts_{t+1}}^a v(s_t)\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/MC10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\\begin{align}\n",
    "q_{k+1}(s,a) &= R_s^a + \\gamma \\sum_{s^\\prime\\in \\it{S}} P_{ss^\\prime}^a \\underset{a^\\prime}{\\operatorname{max}}  q_k(s^\\prime,a^\\prime) \\\\\n",
    "q_{k+1}(s_t,a_t) & \\leftarrow (1-\\alpha)q_k(s_t,a_t) + \\alpha(R_{t+1} + \\gamma q_k(s_{t+1},a_{t+1}))\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/TD10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### $\\epsilon$-greedy\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/MC11.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### On-policy vs Off-policy\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/Off1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Q-learning\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/off6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/off7.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/off9.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Value Function Approximation (Non-tabular value function learning)\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx3.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx4.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx8.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx21.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/main-qimg-01c15f01cf6a56c19313c2791d5a9ae1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Deep Q Network\n",
    "---\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/apx20.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/90-6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/Screen-Shot-2015-12-21-at-11.08.53-AM.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/dqn16.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"https://dnddnjs.gitbooks.io/rl/content/dqn17.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
