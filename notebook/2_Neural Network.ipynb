{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network\n",
    "---\n",
    "\n",
    "\\begin{align} y = f(w^T\\phi(x)) \\end{align}\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 \\end{align}\n",
    "\n",
    "<img src=\"https://cdnpythonmachinelearning.azureedge.net/wp-content/uploads/2017/09/Single-Perceptron.png?x31195\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relation with previous model\n",
    "---\n",
    "\n",
    "\\begin{align} y = f(w^T\\phi(x)) \\end{align}\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 \\end{align}\n",
    "\n",
    "\n",
    "### Perceptron Rule for SVM\n",
    "\n",
    "\\begin{align} y = sgn(w^{*T}x+b^*) \\end{align}\n",
    "\n",
    "\\begin{align} E_p(w) = -\\sum y_iw^Tx_i \\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/2012mdsp-pr13supportvectormachine-130701022429-phpapp02/95/2012mdsp-pr13-support-vector-machine-5-638.jpg?cb=1372645656\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Function\n",
    "---\n",
    "\n",
    "### Probabilistic Supervised Learning - Logistic Regression\n",
    "\n",
    "![](http://www.saedsayad.com/images/LogReg_1.png)\n",
    "\n",
    "\\begin{align} p(y = 1 |x; \\theta) = \\sigma(\\theta^Tx) \\end{align}\n",
    "\n",
    "\\begin{align} \\theta_{ML} = \\underset{\\theta}{\\operatorname{argmax}} P(Y|X; \\theta) \\end{align}\n",
    "\n",
    "\n",
    "### Maximum Likelihood Estimation\n",
    "\n",
    "\\begin{align} \\theta_{ML} = \\underset{\\theta}{\\operatorname{argmax}} P(Y|X; \\theta) \\end{align}\n",
    "\n",
    "\\begin{align} P(Y|X; \\theta) = \\displaystyle\\prod_{i=1} P(y_i|x_i; \\theta) \\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\theta_{ML} &= \\underset{\\theta}{\\operatorname{argmin}} \\Big(-\\log P(Y|X; \\theta)\\Big) \\\\\n",
    "            &= \\underset{\\theta}{\\operatorname{argmin}} \\Big(-\\displaystyle\\sum_{i=1}{\\log P(y_i|x_i; \\theta)} \\Big)\n",
    "\\end{align}\n",
    "\n",
    "<center>Negative Log Likelihood loss (NLL_loss)</center>\n",
    "\n",
    "\n",
    "### Information Theory\n",
    "\n",
    "- Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content what so ever.\n",
    "- Less likely events should have higher information content.\n",
    "- Independent events should have additive information.\n",
    "\n",
    "\\begin{align} I(X) = -logP(X) \\end{align}\n",
    "\n",
    "\n",
    "### Shannon's Entropy\n",
    "\n",
    "\\begin{align} H(X) = E_{x \\sim P}[I(X)] \\end{align}\n",
    "\n",
    "<img src=\"https://i.imgur.com/Pynf9sG.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "### Cross-Entropy\n",
    "\n",
    "데이터 분포 P(X) 상에서 추정 확률 분포 Q(X)가 가지는 정보의 기댓값\n",
    "\n",
    "\\begin{align} H(P,Q) = H(P) + D_{KL}(P||Q) \\end{align}\n",
    "\n",
    "\\begin{align} H(P,Q) = E_{x \\sim P}[\\log Q(X)] \\end{align}\n",
    "\n",
    "\n",
    "### Kullback-Leibler divergence, KLD\n",
    "\n",
    "두 확률분포의 차이를 계산하는 데 사용하는 함수. 가지고 있는 데이터의 분포 P(x)와 모델이 추정한 데이터의 분포 Q(x) 간의 차이\n",
    "\n",
    "\\begin{align} D_{KL}(P||Q) = E_{x \\sim P}[\\log\\frac{P(X)}{Q(X)}] \\end{align}\n",
    "\n",
    "\n",
    "### Relation between Cross Entropy Loss and Maximum Likelihood Estimation\n",
    "\n",
    "\\begin{align} H(P,Q) = E_{x \\sim P}[\\log Q(X)] \\end{align}\n",
    "\n",
    "\\begin{align} \n",
    "\\theta_{ML} &= \\underset{\\theta}{\\operatorname{argmax}} H(P_{real},P_{train}) \\\\\n",
    "            &= \\underset{\\theta}{\\operatorname{argmax}} E_{x \\sim P}[\\log Q(X)] \\\\\n",
    "            &\\sim \\underset{\\theta}{\\operatorname{argmax}} \\Big(-\\displaystyle\\sum_{i=1}{\\log P(y_i|x_i; \\theta)} \\Big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back Propagation\n",
    "---\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/lecture11-090302042359-phpapp01/95/lecture11-neural-networks-23-728.jpg?cb=1235978279\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/lecture11-090302042359-phpapp01/95/lecture11-neural-networks-24-1024.jpg?cb=1235978279\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/lecture11-090302042359-phpapp01/95/lecture11-neural-networks-25-1024.jpg?cb=1235978279\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://www.slideshare.net/aorriols/lecture11-neural-networks\">Back Propagation Slide Note</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "### Vanishing Gradient\n",
    "\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/2168AC3758D5CE0C33\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*RD0lIYqB5L2LrI2VTIZqGw.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalization\n",
    "---\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/random-170910154045/95/-53-638.jpg?cb=1505089848\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "### Learning Theory - VC Dimension (Vapnik–Chervonenkis dimension)\n",
    "\n",
    "<img src=\"http://www.guochaoping.top/wp-content/uploads/2016/11/Sample_complexity_tradeoff.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "\\begin{align} error_{test} \\leq error_{training} + \\sqrt{\\frac{1}{N}\\Big[D\\Big(\\log\\Big(\\frac{2N}{D}\\Big)+1\\Big)-\\log\\Big(\\frac{1}{\\delta}\\Big)\\Big]} \\end{align}\n",
    "\n",
    "\\begin{align} d_{VC} = \\frac{D}{N}, \\:\\: D:\\:model\\:complexity, \\:\\: N:\\:number\\:of\\:samples \\end{align}\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Screen-Shot-2018-04-03-at-7.52.01-PM-e1522832332857.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\\begin{align} J(\\theta) = \\sum_{i=1} (h_\\theta(x_i)-y_i)^2 + \\lambda ||\\theta||^2 \\end{align}\n",
    "\n",
    "\\begin{align} h_\\theta(x_i) = \\theta^Tx_i + b \\end{align}\n",
    "\n",
    "\\begin{align} \\theta^* = \\arg\\min_\\theta J(\\theta) \\end{align}\n",
    "\n",
    "\n",
    "### Drop Out\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/random-170910154045/95/-55-638.jpg?cb=1505089848\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*vW3KGPp_w0wN6k3gYVlVHA.jpeg\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<center>\n",
    "    <a href=\"https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced\">Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2</a>\n",
    "</center>\n",
    "\n",
    "\n",
    "### Hyperparamter Training - Cross Validation\n",
    "\n",
    "<img src=\"http://www.cs.nthu.edu.tw/~shwu/courses/ml/labs/08_CV_Ensembling/fig-holdout.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "https://ars.els-cdn.com/content/image/1-s2.0-S016516841500290X-gr1.jpg\n",
    "<img src=\"https://i.stack.imgur.com/1fXzJ.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Techniques\n",
    "---\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "\n",
    "### Curriculum Learning\n",
    "\n",
    "\n",
    "### Supervised PreTraining\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
